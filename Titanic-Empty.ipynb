{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Prediction on Titanic\n",
    "The purpose of this notebook is to explore the data from the Titanic and be able to predict :\n",
    "\n",
    "Who will survive, who will die ?\n",
    "\n",
    "## Data preparation\n",
    "\n",
    "Prior to loading the data, and to manipulate them, some preparation tasks have to be performed :\n",
    "* importing the libraries\n",
    "* defining functions for the graphs definitions\n",
    "\n",
    "### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Regular expressions\n",
    "import re as re\n",
    "\n",
    "# Handle table-like data and matrices\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Modelling Algorithms\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "import xgboost as xgb\n",
    "\n",
    "# Modelling Helpers\n",
    "from sklearn.preprocessing import Imputer , Normalizer , scale\n",
    "from sklearn.cross_validation import train_test_split , StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "\n",
    "# Configure visualisations\n",
    "%matplotlib inline\n",
    "mpl.style.use( 'ggplot' )\n",
    "#sns.set_style( 'white' )\n",
    "pylab.rcParams[ 'figure.figsize' ] = 8, 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Define graph functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def plot_distribution(df, var, target, **kwargs ):\n",
    "    row = kwargs.get('row', None)\n",
    "    col = kwargs.get('col', None)\n",
    "    facet = sns.FacetGrid(df, hue = target, aspect = 4 ,row = row, col = col)\n",
    "    facet.map(sns.kdeplot, var, shade = True)\n",
    "    facet.set(xlim = (0, df[var].max()))\n",
    "    facet.add_legend()\n",
    "\n",
    "def plot_categories(df, cat, target, **kwargs):\n",
    "    row = kwargs.get('row', None)\n",
    "    col = kwargs.get('col', None)\n",
    "    facet = sns.FacetGrid(df, row = row, col = col)\n",
    "    facet.map(sns.barplot, cat, target)\n",
    "    facet.add_legend()\n",
    "\n",
    "def plot_correlation_map(df):\n",
    "    corr = titanic.corr()\n",
    "    _, ax = plt.subplots(figsize = (22, 20))\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap = True)\n",
    "    _ = sns.heatmap(\n",
    "        corr, \n",
    "        cmap = cmap,\n",
    "        square=True, \n",
    "        cbar_kws={'shrink' : .9}, \n",
    "        ax = ax, \n",
    "        annot = True, \n",
    "        annot_kws = {'fontsize' : 12}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Loading the datasets\n",
    "The dataset is composed of 2 csv files : the train and the test data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Lets store the PassengerId for an easier access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "PassengerId = test['PassengerId']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's merge de train and test data frame to have a full vision of the Titanic passengers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full = train.append(test, ignore_index = True)\n",
    "titanic = full[:89 ]\n",
    "\n",
    "print ('Datasets:', 'full:', full.shape, 'titanic:', titanic.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data exploration\n",
    "### Data Overview\n",
    "Let's start looking at the first lines of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The dataset is composed of 1309 lines with a set of 12 variables.\n",
    "\n",
    "The variables are described below:\n",
    "* Survived: Survived (1) or died (0)\n",
    "* Pclass: Passenger's class\n",
    "* Name: Passenger's name\n",
    "* Sex: Passenger's sex\n",
    "* Age: Passenger's age\n",
    "* SibSp: Number of siblings/spouses aboard\n",
    "* Parch: Number of parents/children aboard\n",
    "* Ticket: Ticket number\n",
    "* Fare: Fare\n",
    "* Cabin: Cabin\n",
    "* Embarked: Port of embarkation\n",
    "\n",
    "Let's have a look at the data descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "titanic.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data preparation and cleaning\n",
    "As there are NaN values and strings, lets do some transformation to use all the possible features of the dataset.\n",
    "we can also add some features such as the family size or if the people is alone.\n",
    "### Adding features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Feature that tells whether a passenger had a cabin or not \n",
    "full['Has_Cabin'] = full[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n",
    "\n",
    "# Create new feature FamilySize as a combination of SibSp and Parch\n",
    "full['FamilySize'] = full[\"SibSp\"] + full[\"Parch\"] + 1\n",
    "\n",
    "# Create new feature IsAlone from FamilySize\n",
    "full['IsAlone'] = full[\"FamilySize\"].apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "# Lets add a column for the sex in binary format with 0 for male and 1 for female\n",
    "full['SexBin'] = full['Sex'].apply(lambda x: 0 if x == 'male' else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In the Name column some interresting the title can be extracted to create a new feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_title(name):\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "full['Title'] = full['Name'].apply(lambda x: get_title(x))\n",
    "\n",
    "\n",
    "print(pd.crosstab(full['Title'], full['Sex']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Lets categorize the title in order to have a clearer feature. This will help also to separate the passengers from the crew members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full['Title'] = full['Title'].replace('Mlle', 'Miss')\n",
    "full['Title'] = full['Title'].replace('Ms', 'Miss')\n",
    "full['Title'] = full['Title'].replace('Mme', 'Mrs')\n",
    "full['Title'] = full['Title'].replace(['Lady', 'Countess'], 'Mrs')\n",
    "full['Title'] = full['Title'].replace(['Capt', 'Col', 'Dr', 'Major'], 'Crew')\n",
    "full['Title'] = full['Title'].replace(['Don', 'Rev', 'Sir', 'Jonkheer'], 'Mr')\n",
    "\n",
    "full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Lets extract Cabin category information from the Cabin number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cabin = pd.DataFrame()\n",
    "\n",
    "# replacing missing cabins with U (for Uknown)\n",
    "cabin['Cabin'] = full['Cabin'].fillna('U')\n",
    "full['Cabin'].fillna('U')\n",
    "\n",
    "# mapping each Cabin value with the cabin letter\n",
    "cabin['Cabin'] = cabin['Cabin'].map(lambda c: c[0])\n",
    "\n",
    "# dummy encoding ...\n",
    "cabin = pd.get_dummies(cabin['Cabin'], prefix = 'Cabin')\n",
    "\n",
    "cabin.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Then merge de cabin data frame with the full one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full = full.merge(cabin, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Lets extract ticket class from ticket number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Extraction of each prefix of the ticket, returns 'XXX' if no prefix (i.e the ticket is a digit)\n",
    "def cleanTicket(ticket):\n",
    "    ticket = ticket.replace('.', '' )\n",
    "    ticket = ticket.replace('/', '' )\n",
    "    ticket = ticket.split()\n",
    "    ticket = map(lambda t: t.strip(), ticket)\n",
    "    ticket = list(filter(lambda t: not t.isdigit(), ticket))\n",
    "    if len(ticket) > 0:\n",
    "        return ticket[0]\n",
    "    else: \n",
    "        return 'XXX'\n",
    "\n",
    "ticket = pd.DataFrame()\n",
    "\n",
    "# Extracting dummy variables from tickets:\n",
    "ticket['Ticket'] = full['Ticket'].map(cleanTicket)\n",
    "ticket = pd.get_dummies(ticket['Ticket'], prefix = 'Ticket')\n",
    "\n",
    "ticket.shape\n",
    "ticket.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Then merge de ticket data with the full data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full = full.merge(ticket, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Removing NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Lets identify the columns with NULL values\n",
    "full.info()\n",
    "\n",
    "# Remove all NULLS in the Embarked column assuming that all passengers with NaN values boarded at Southampton.\n",
    "full['Embarked'].fillna('S')\n",
    "\n",
    "# Fare also has some missing value and we will replace it with the median.\n",
    "full['Fare'] = full['Fare'].fillna(full['Fare'].median())\n",
    "\n",
    "# Age also have NULL values. Let's fill the NaN values with random numbers between (mean - std) and (mean + std)\n",
    "age_avg = full['Age'].mean()\n",
    "age_std = full['Age'].std()\n",
    "\n",
    "full['Age'] = full['Age'].fillna(np.random.randint(age_avg - age_std, age_avg + age_std, size=1)[0])\n",
    "\n",
    "# Create a new column for the cabin in a binary format with 1 if the passenger has a cabin assigned to him\n",
    "full['HasCabin'] = full['Cabin'].apply(lambda x: 1 if pd.notnull(x) else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Feature data correlation\n",
    "To see which variables are important, let's graph a heat map of correlation without the ticket and cabin information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "titanic = full[:891]\n",
    "titanic_map = titanic.drop(titanic.columns[17:], axis=1, inplace=True)\n",
    "plot_correlation_map(titanic_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The Correlation plot shows that there are not too many features strongly correlated with one another. \n",
    "This point will in interresting for the feeding of the learnong modelthis means that there isn't much redundant or superfluous data in our training set and that each feature carries with it some unique information. The most correlated features is Parch (Parents and Children)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data visualization\n",
    "Let's explore the relationship between the different features and the survival of passenger.\n",
    "### Survival rate by age\n",
    "The graph below shows the relationship between the age and the survived variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plot_distribution(titanic, var = 'Age', target = 'Survived', row = 'Sex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This distribution marks clearly a high probability for each sex people to have the capability to survive. Young male (below 15) have a higher chance to survive rather than older one (15 to 32 years old).\n",
    "For female, the probability to survive is when you are between 25 and 41, then older than 50 years old.\n",
    "\n",
    "Lets split by 5 the age by sex to have an overview of the probability to survive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "titanic['CategoricalAge'] = pd.cut(titanic['Age'], 5)\n",
    "\n",
    "print (titanic[['CategoricalAge', 'Survived']].groupby(['CategoricalAge'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Whatever is the sex, young people (below 16 years old) have a higher chance to survive whereas the older than 64 years have a really high probability not to survive.\n",
    "### Survival rate by Fare\n",
    "The graph below shows the relationship between the fare of passangers and the survived variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plot_distribution(titanic, var = 'Fare', target = 'Survived', row = 'Sex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Lets categorize the fare into 4 ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "titanic['CategoricalFare'] = pd.qcut(titanic['Fare'], 5)\n",
    "print (titanic[['CategoricalFare', 'Survived']].groupby(['CategoricalFare'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This distribution graph shows that whatever is your sex (male or female), your probability to died is higher when your fare is low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Survival rate by Embarkment place\n",
    "The graph below shows the relationship between the embarkment place and the survival probability.\n",
    "The embarkment places are the following ones :\n",
    "* C = Cherbourg\n",
    "* Q = Queenstown\n",
    "* S = Southampton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plot_categories(titanic, cat = 'Embarked' ,target = 'Survived')\n",
    "print (titanic[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Passengers who have embarked at Cherbourg have a higher chance to survive rather than the others. Southampton passengers are the one who have the most chance not to survive.\n",
    "### Survival rate by Sex\n",
    "The graph below shows the survival rate by Sex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(titanic[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean())\n",
    "plot_categories(titanic, cat = 'Sex', target = 'Survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Being a male or a female has an impact on the survival rate.\n",
    "### Survival rate by family size\n",
    "The graph below shows the survival rate by family size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(titanic[[\"FamilySize\", \"Survived\"]].groupby(['FamilySize'], as_index=False).mean())\n",
    "plot_categories(titanic, cat = 'FamilySize', target = 'Survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's have a look at the single family with the below graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(titanic[[\"IsAlone\", \"Survived\"]].groupby(['IsAlone'], as_index=False).mean())\n",
    "plot_categories(titanic, cat = 'IsAlone', target = 'Survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Having a family not more than 4 people provide a larger chance to survive than the other. The largest family has no chance according to the data set.\n",
    "### Survival rate by Passenger class\n",
    "The graph below shows the survival rate by passenger class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(titanic[[\"Pclass\", \"Survived\"]].groupby(['Pclass'], as_index=False).mean())\n",
    "plot_categories(titanic, cat = 'Pclass', target = 'Survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The higher is the class the more chance to survive is.\n",
    "### Survival rate by cabin\n",
    "The graph below shows the survival rate whereas the passenger has a cabin assigned to him or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Use all the columns\n",
    "titanic = full[:891]\n",
    "print(titanic[[\"HasCabin\", \"Survived\"]].groupby(['HasCabin'], as_index=False).mean())\n",
    "plot_categories(titanic, cat = 'HasCabin', target = 'Survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Having a cabin drives to a higher chance to survie than not having one. Lets' do a correlation map only focussed on the cabin type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "titanic = full[:891]\n",
    "titanic_map = titanic.drop(titanic.columns[26:], axis=1, inplace=True)\n",
    "titanic_map = titanic.drop(titanic.columns[:10], axis=1, inplace=True)\n",
    "titanic_map = titanic.drop(titanic.columns[1:6], axis=1, inplace=True)\n",
    "\n",
    "plot_correlation_map(titanic_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Some correlation can be made also with the cabin type. However, with the number of unknown cabin type (Cabin_U), this feature is not one of the most relevant.\n",
    "### Survival rate by title\n",
    "The graph below shows the survival rate by title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(titanic[[\"Title\", \"Survived\"]].groupby(['Title'], as_index=False).mean())\n",
    "plot_categories(titanic, cat = 'Title', target = 'Survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The title is an interresting feature as it is also summaryzing some information compared to the previous one such as sex, age.\n",
    "Being a man give less chance to survice than a female. It is also observed that young men have a higher chance to survive than aldults one.\n",
    "This feature provide also a vision of the survival rate of the crew members.\n",
    "# Modeling\n",
    "We will now select a model we would like to try then use the training dataset to train this model and thereby check the performance of the model using the test set.\n",
    "## Final cleaning\n",
    "Prior to the modeling selection, lets do some final cleaning and feature arrangment to be able to manage them all. The features which needs to be adjusted is:\n",
    "* Embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create a new variable for every unique value of Embarked\n",
    "embarked = pd.get_dummies(full['Embarked'], prefix='Embarked')\n",
    "embarked.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Then merge the generated embarked data with the full data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full = full.merge(embarked, left_index=True, right_index=True)\n",
    "full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we will remove the strings columns and all the tickets columns:\n",
    "* Cabin\n",
    "* Embarked\n",
    "* Name\n",
    "* Sex\n",
    "* Ticket\n",
    "* Title\n",
    "* PassengerId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full.drop(full.columns[[1, 2, 4, 6, 8, 11, 16]], axis=1, inplace = True)\n",
    "full.drop(full.columns[19:], axis = 1, inplace = True)\n",
    "full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create datasets\n",
    "Lets create training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Selecting the right model\n",
    "First lets initialize some parameters which are going to be used later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Some useful parameters which will come in handy later on\n",
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]\n",
    "SEED = 0 # for reproducibility\n",
    "NFOLDS = 5 # set folds for out-of-fold prediction\n",
    "kf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\n",
    "\n",
    "# Class to extend the Sklearn classifier\n",
    "class SklearnHelper(object):\n",
    "    def __init__(self, clf, seed=0, params=None):\n",
    "        params['random_state'] = seed\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x)\n",
    "    \n",
    "    def fit(self,x,y):\n",
    "        return self.clf.fit(x,y)\n",
    "    \n",
    "    def feature_importances(self,x,y):\n",
    "        print(self.clf.fit(x,y).feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will use the predictions of base classifiers as input for training to a second-level model. However one cannot simply train the base models on the full training data, generate predictions on the full test set and then output these for the second-level training. This runs the risk of your base model predictions already having \"seen\" the test set and therefore overfitting when feeding these predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_oof(clf, x_train, y_train, x_test):\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((NFOLDS, ntest))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y_train[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "\n",
    "        clf.train(x_tr, y_tr)\n",
    "\n",
    "        oof_train[test_index] = clf.predict(x_te)\n",
    "        oof_test_skf[i, :] = clf.predict(x_test)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Generating the first level of model\n",
    "The following models will be tested :\n",
    "* Random Forest classifier\n",
    "* Extra Trees classifier\n",
    "* AdaBoost classifer\n",
    "* Gradient Boosting classifer\n",
    "* Support Vector Machine\n",
    "* Decision Tree Classifier\n",
    "* Logistic Regression\n",
    "\n",
    "Parameters\n",
    "\n",
    "Just a quick summary of the parameters that we will be listing here for completeness:\n",
    "* n_jobs : Number of cores used for the training process. If set to -1, all cores are used.\n",
    "* n_estimators : Number of classification trees in your learning model ( set to 10 per default)\n",
    "* max_depth : Maximum depth of tree, or how much a node should be expanded. Beware if set to too high a number would run the risk of overfitting as one would be growing the tree too deep\n",
    "* verbose : Controls whether you want to output any text during the learning process. A value of 0 suppresses all text while a value of 3 outputs the tree learning process at every iteration.\n",
    "\n",
    "Lets initalialize the parameters for the different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Random Forest parameters\n",
    "rf_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators': 500,\n",
    "     'warm_start': True, \n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 6,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_features' : 'sqrt',\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Extra Trees Parameters\n",
    "et_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators':500,\n",
    "    #'max_features': 0.5,\n",
    "    'max_depth': 8,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# AdaBoost parameters\n",
    "ada_params = {\n",
    "    'n_estimators': 500,\n",
    "    'learning_rate' : 0.75\n",
    "}\n",
    "\n",
    "# Gradient Boosting parameters\n",
    "gb_params = {\n",
    "    'n_estimators': 500,\n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Support Vector Classifier parameters \n",
    "svc_params = {\n",
    "    'kernel' : 'linear',\n",
    "    'C' : 0.025\n",
    "    }\n",
    "\n",
    "# Decision Tree parameters\n",
    "dt_params = {\n",
    "    'criterion': 'gini',\n",
    "    'max_features' : 'sqrt'\n",
    "}\n",
    "\n",
    "# Logistic Regression parameters\n",
    "lr_params = {\n",
    "    'warm_start': True,\n",
    "    'verbose': 0,\n",
    "    'n_jobs': 4\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Create the objects which represents the models using the class defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rf = SklearnHelper(clf = RandomForestClassifier, seed = SEED, params = rf_params)\n",
    "et = SklearnHelper(clf = ExtraTreesClassifier, seed = SEED, params = et_params)\n",
    "ada = SklearnHelper(clf = AdaBoostClassifier, seed = SEED, params = ada_params)\n",
    "gb = SklearnHelper(clf = GradientBoostingClassifier, seed = SEED, params = gb_params)\n",
    "svc = SklearnHelper(clf = SVC, seed = SEED, params = svc_params)\n",
    "dt = SklearnHelper(clf = DecisionTreeClassifier, seed = SEED, params = dt_params)\n",
    "lr = SklearnHelper(clf = LogisticRegression, seed = SEED, params = lr_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Creation of the test and train data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create all datasets that are necessary to train, validate and test models\n",
    "train = full[0:891]\n",
    "train_valid = train['Survived']\n",
    "test = full[891:]\n",
    "# Create Numpy arrays of train, test and target (Survived) dataframes to feed into our models\n",
    "y_train = train['Survived'].ravel()\n",
    "train = train.drop(['Survived'], axis = 1)\n",
    "test = test.drop(['Survived'], axis = 1)\n",
    "x_train = train.values # Creates an array of the train data\n",
    "x_test = test.values # Creates an array of the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Check data validation before launching the predictions\n",
    "As the input shouldn't contains NaN, inifity or a too large value, let's check that the data set are conform to this by using the function _assert_all_finite().\n",
    "No error message should be raise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def _assert_all_finite(X):\n",
    "    \"\"\"Like assert_all_finite, but only for ndarray.\"\"\"\n",
    "    X = np.asanyarray(X)\n",
    "    # First try an O(n) time, O(1) space solution for the common case that\n",
    "    # everything is finite; fall back to O(n) space np.isfinite to prevent\n",
    "    # false positives from overflow in sum method.\n",
    "    if (X.dtype.char in np.typecodes['AllFloat'] and not np.isfinite(X.sum())\n",
    "            and not np.isfinite(X).all()):\n",
    "        raise ValueError(\"Input contains NaN, infinity\"\n",
    "                         \" or a value too large for %r.\" % X.dtype)\n",
    "        \n",
    "_assert_all_finite(y_train)\n",
    "_assert_all_finite(x_train)\n",
    "_assert_all_finite(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Output of the First level Predictions\n",
    "We will use the Out-of-Fold prediction function we defined earlier to generate our first level predictions. This will take some time to acheive this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create our OOF train and test predictions. These base results will be used as new features\n",
    "print(\"Training Extra Trees\")\n",
    "et_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\n",
    "\n",
    "print(\"Training Random Forest\")\n",
    "rf_oof_train, rf_oof_test = get_oof(rf, x_train, y_train, x_test) # Random Forest\n",
    "\n",
    "print(\"Training AdaBoost\")\n",
    "ada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \n",
    "\n",
    "print(\"Training Gradient Boost\")\n",
    "gb_oof_train, gb_oof_test = get_oof(gb, x_train, y_train, x_test) # Gradient Boost\n",
    "\n",
    "print(\"Training Support Vector Classifier\")\n",
    "svc_oof_train, svc_oof_test = get_oof(svc, x_train, y_train, x_test) # Support Vector Classifier\n",
    "\n",
    "print (\"Training Decision Tree Classifier\")\n",
    "dt_oof_train, dt_oof_test = get_oof(dt, x_train, y_train, x_test) # Decision Tree Classifier\n",
    "\n",
    "print(\"Training Logistic Regression\")\n",
    "lr_oof_train, lr_oof_test = get_oof(lr, x_train, y_train, x_test) # Logistic Regression\n",
    "\n",
    "print(\"Training is complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Feature importances generated from the different classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now having learned our the first-level classifiers, we can utilise a very nifty feature of the Sklearn models and that is to output the importances of the various features in the training and test sets with one very simple line of code.\n",
    "As per the Sklearn documentation, most of the classifiers are built in with an attribute which returns feature importances by simply typing in .featureimportances. Therefore we will invoke this very useful attribute via our function earliand plot the feature importances as such.\n",
    "\n",
    "The following modelling algorithm don't have any feature importance method\n",
    "* Support Vector Classifier\n",
    "* Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"Extra Tree feature\")\n",
    "et_feature = et.feature_importances(x_train, y_train)\n",
    "print(\"Random Forest feature\")\n",
    "rf_feature = rf.feature_importances(x_train, y_train)\n",
    "print(\"AdaBoost feature\")\n",
    "ada_feature = ada.feature_importances(x_train, y_train)\n",
    "print(\"Gradient Boost feature\")\n",
    "gb_feature = gb.feature_importances(x_train, y_train)\n",
    "print(\"Decision Tree feature\")\n",
    "dt_feature = dt.feature_importances(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Lets store the features importance in an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "et_features = [ 0.05011595,  0.04284341,  0.01921525,  0.110844,    0.02640853,  0.06698562,\n",
    "  0.03532985,  0.03189318,  0.50049825,  0.00339117,  0.01068302,  0.00837521,\n",
    "  0.00637922,  0.01190591,  0.00395718,  0.0025294,   0.,          0.06864485]\n",
    "rf_features = [ 0.11046156,  0.17613965,  0.02475541,  0.11329302,  0.02709934,  0.04966605,\n",
    "  0.05802088,  0.01890127,  0.33654778,  0.00299868,  0.00953418,  0.00731309,\n",
    "  0.00395437,  0.01009517,  0.00173278,  0.00089213,  0.,          0.04859466]\n",
    "ada_features = [ 0.256,  0.604,  0.032,  0.002,  0.006,  0.002,  0.056,  0.,     0.018,  0.,     0.002,\n",
    "  0.002,  0.004,  0.006,  0.,     0.004,  0.002,  0.004]\n",
    "gb_features = [  3.65903579e-01,   4.64744988e-01,   8.72998258e-03,   2.40536109e-02,\n",
    "   1.66487753e-02,   6.83559768e-03,   2.52165063e-02,   4.89974450e-03,\n",
    "   4.91318415e-02,   4.08279904e-03,   4.04680366e-03,   1.09309351e-02,\n",
    "   2.62153988e-03,   4.50583486e-03,   1.36484913e-03,   2.57626670e-04,\n",
    "   0.00000000e+00,   6.02498557e-03]\n",
    "dt_features = [ 0.28184935,  0.22410379,  0.02291037,  0.0276042,   0.03999138,  0.0498649,\n",
    "  0.01866463,  0.00782725,  0.30692013,  0.00136774,  0.00356788,  0.0054007,\n",
    "  0.00134339,  0.00729523,  0.,          0.00128904,  0.,          0.        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Create a dataframe from the lists containing the feature importance data for easy plotting via the Plotly package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cols = train.columns.values\n",
    "# Create a dataframe with features\n",
    "feature_dataframe = pd.DataFrame( {'features': cols,\n",
    "    'Random Forest feature importances': rf_features,\n",
    "    'Extra Trees feature importances': et_features,\n",
    "    'AdaBoost feature importances': ada_features,\n",
    "    'Gradient Boost feature importances': gb_features,\n",
    "    'Decision Tree feature importances': dt_features\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Interactive feature importances via Plotly scatterplots\n",
    "Lets graph Random Forest feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "trace = go.Scatter(\n",
    "    y = feature_dataframe['Random Forest feature importances'].values,\n",
    "    x = feature_dataframe['features'].values,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        sizemode = 'diameter',\n",
    "        sizeref = 1,\n",
    "        size = 25,\n",
    "        color = feature_dataframe['Random Forest feature importances'].values,\n",
    "        colorscale='Portland',\n",
    "        showscale=True\n",
    "    ),\n",
    "    text = feature_dataframe['features'].values\n",
    ")\n",
    "data = [trace]\n",
    "\n",
    "layout= go.Layout(\n",
    "    autosize= True,\n",
    "    title= 'Random Forest Feature Importance',\n",
    "    hovermode= 'closest',\n",
    "    yaxis=dict(\n",
    "        title= 'Feature Importance',\n",
    "        ticklen= 5,\n",
    "        gridwidth= 2\n",
    "    ),\n",
    "    showlegend= False\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig,filename='fig1_rf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Lets graph Extra trees features importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "trace = go.Scatter(\n",
    "    y = feature_dataframe['Extra Trees feature importances'].values,\n",
    "    x = feature_dataframe['features'].values,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        sizemode = 'diameter',\n",
    "        sizeref = 1,\n",
    "        size = 25,\n",
    "        color = feature_dataframe['Extra Trees feature importances'].values,\n",
    "        colorscale='Portland',\n",
    "        showscale=True\n",
    "    ),\n",
    "    text = feature_dataframe['features'].values\n",
    ")\n",
    "data = [trace]\n",
    "\n",
    "layout= go.Layout(\n",
    "    autosize= True,\n",
    "    title= 'Extra Trees Feature Importance',\n",
    "    hovermode= 'closest',\n",
    "    yaxis=dict(\n",
    "        title= 'Feature Importance',\n",
    "        ticklen= 5,\n",
    "        gridwidth= 2\n",
    "    ),\n",
    "    showlegend= False\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig,filename='fig1_et')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Lets graph Ada boost feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "trace = go.Scatter(\n",
    "    y = feature_dataframe['AdaBoost feature importances'].values,\n",
    "    x = feature_dataframe['features'].values,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        sizemode = 'diameter',\n",
    "        sizeref = 1,\n",
    "        size = 25,\n",
    "        color = feature_dataframe['AdaBoost feature importances'].values,\n",
    "        colorscale='Portland',\n",
    "        showscale=True\n",
    "    ),\n",
    "    text = feature_dataframe['features'].values\n",
    ")\n",
    "data = [trace]\n",
    "\n",
    "layout= go.Layout(\n",
    "    autosize= True,\n",
    "    title= 'AdaBoost Feature Importance',\n",
    "    hovermode= 'closest',\n",
    "    yaxis=dict(\n",
    "        title= 'Feature Importance',\n",
    "        ticklen= 5,\n",
    "        gridwidth= 2\n",
    "    ),\n",
    "    showlegend= False\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig,filename='fig1_gf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Lets graph Gradient boost feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "trace = go.Scatter(\n",
    "    y = feature_dataframe['Gradient Boost feature importances'].values,\n",
    "    x = feature_dataframe['features'].values,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        sizemode = 'diameter',\n",
    "        sizeref = 1,\n",
    "        size = 25,\n",
    "        color = feature_dataframe['Gradient Boost feature importances'].values,\n",
    "        colorscale='Portland',\n",
    "        showscale=True\n",
    "    ),\n",
    "    text = feature_dataframe['features'].values\n",
    ")\n",
    "data = [trace]\n",
    "\n",
    "layout= go.Layout(\n",
    "    autosize= True,\n",
    "    title= 'Gradient Boosting Feature Importance',\n",
    "    hovermode= 'closest',\n",
    "    yaxis=dict(\n",
    "        title= 'Feature Importance',\n",
    "        ticklen= 5,\n",
    "        gridwidth= 2\n",
    "    ),\n",
    "    showlegend= False\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig,filename='fig1_gb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Lets graph Decision Tree feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "trace = go.Scatter(\n",
    "    y = feature_dataframe['Decision Tree feature importances'].values,\n",
    "    x = feature_dataframe['features'].values,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        sizemode = 'diameter',\n",
    "        sizeref = 1,\n",
    "        size = 25,\n",
    "        color = feature_dataframe['Decision Tree feature importances'].values,\n",
    "        colorscale='Portland',\n",
    "        showscale=True\n",
    "    ),\n",
    "    text = feature_dataframe['features'].values\n",
    ")\n",
    "data = [trace]\n",
    "\n",
    "layout= go.Layout(\n",
    "    autosize= True,\n",
    "    title= 'Decision Tree Feature Importance',\n",
    "    hovermode= 'closest',\n",
    "    yaxis=dict(\n",
    "        title= 'Feature Importance',\n",
    "        ticklen= 5,\n",
    "        gridwidth= 2\n",
    "    ),\n",
    "    showlegend= False\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig,filename='fig1_gb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Lets calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "feature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\n",
    "feature_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Plot the main important features\n",
    "The graph below shows the mean feature importance accross all the tested models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y = feature_dataframe['mean'].values\n",
    "x = feature_dataframe['features'].values\n",
    "data = [go.Bar(\n",
    "            x= x,\n",
    "             y= y,\n",
    "            width = 0.5,\n",
    "            marker=dict(\n",
    "               color = feature_dataframe['mean'].values,\n",
    "            colorscale='Portland',\n",
    "            showscale=True,\n",
    "            reversescale = False\n",
    "            ),\n",
    "            opacity=0.6\n",
    "        )]\n",
    "\n",
    "layout= go.Layout(\n",
    "    autosize= True,\n",
    "    title= 'Barplots of Mean Feature Importance',\n",
    "    hovermode= 'closest',\n",
    "    yaxis=dict(\n",
    "        title= 'Feature Importance',\n",
    "        ticklen= 5,\n",
    "        gridwidth= 2\n",
    "    ),\n",
    "    showlegend= False\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='bar-direct-labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Second-Level Predictions from the First-level Output\n",
    "#### First-level output as new features\n",
    "Having now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "base_predictions_train = pd.DataFrame( {\n",
    "    'RandomForest': rf_oof_train.ravel(),\n",
    "    'ExtraTrees': et_oof_train.ravel(),\n",
    "    'AdaBoost': ada_oof_train.ravel(),\n",
    "    'GradientBoost': gb_oof_train.ravel(),\n",
    "    'DecisionTree': dt_oof_train.ravel()\n",
    "    })\n",
    "base_predictions_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Correlation Heatmap of the Second Level Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    go.Heatmap(\n",
    "        z= base_predictions_train.astype(float).corr().values ,\n",
    "        x=base_predictions_train.columns.values,\n",
    "        y= base_predictions_train.columns.values,\n",
    "          colorscale='Portland',\n",
    "            showscale=True,\n",
    "            reversescale = True\n",
    "    )\n",
    "]\n",
    "py.iplot(data, filename='corr-heatmap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Lets now concatenate and join both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\n",
    "x_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Second level learning model via XGBoost\n",
    "XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gbm = xgb.XGBClassifier(\n",
    "    #learning_rate = 0.02,\n",
    "    n_estimators= 2000,\n",
    "    max_depth= 4,\n",
    "    min_child_weight= 2,\n",
    "    #gamma=1,\n",
    "    gamma=0.9,                        \n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    nthread= -1,\n",
    "    scale_pos_weight=1).fit(x_train, y_train)\n",
    "predictions = gbm.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Producing the Submission file\n",
    "Finally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Generate Submission File \n",
    "StackingSubmission = pd.DataFrame({'PassengerId': PassengerId, 'Survived': predictions})\n",
    "\n",
    "# Correcting the output format\n",
    "StackingSubmission['Survived'].astype(int)\n",
    "\n",
    "# Generating the CSV file\n",
    "StackingSubmission.to_csv(\"StackingSubmission3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Steps for Further Improvement\n",
    "As a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels.\n",
    "\n",
    "Some additional steps that may be taken to improve one's score could be:\n",
    "* Implementing a good cross-validation strategy in training the models to find optimal parameter values\n",
    "* Introduce a greater variety of base models for learning. The more uncorrelated the results, the better the final score. (commented ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
